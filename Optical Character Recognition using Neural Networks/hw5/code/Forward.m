function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer pre activations in 'act_a', and the hidden layer post
% activations in 'act_h'.

n=length(W);
act_a={};
act_h={};

for i=1:n   
    if i==1
    temp_a= W{i}*X+b{i};    
    act_a(i)=mat2cell(temp_a,size(temp_a,1));
    temp_h= 1./(1+exp(-act_a{i}));
    act_h(i)= mat2cell(temp_h,size(temp_h,1));
    elseif i>1 && i~=n
    temp_a= W{i}*act_h{i-1}+b{i};
    act_a(i)=mat2cell(temp_a,size(temp_a,1));
    temp_h= 1./(1+exp(-act_a{i}));
    act_h(i)=mat2cell(temp_h,size(temp_h,1));
    else
    temp_a= W{i}*act_h{i-1}+b{i};
    act_a(i)= mat2cell(temp_a,size(temp_a,1));
    s=sum(exp(act_a{i}));
    temp_h= exp(act_a{i})/s;
    act_h(i)= mat2cell(temp_h,size(temp_h,1));
    output=act_h{i};
    end
end
 
    
end
